{"meta":{"title":"IT猿博客","subtitle":"努力的意义是不断的优化自己；     努力的意义是当下的一点一滴；     努力是为了不让热爱的世界拱手相让于人。","description":"天行健，君子以自强不息","author":"夏之颜","url":"http://yoursite.com"},"pages":[{"title":"关于","date":"2019-11-02T03:19:51.000Z","updated":"2019-11-03T14:49:45.416Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-11-02T03:17:54.000Z","updated":"2019-11-02T03:20:37.960Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-11-02T03:19:28.000Z","updated":"2019-11-03T14:45:27.450Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"归档","date":"2019-11-02T03:18:42.000Z","updated":"2019-11-03T14:49:08.221Z","comments":true,"path":"archives/index.html","permalink":"http://yoursite.com/archives/index.html","excerpt":"","text":""}],"posts":[{"title":"分布式文件系统HDFS","slug":"Hadoop-HDFS","date":"2019-11-01T15:45:54.077Z","updated":"2019-11-02T12:05:04.261Z","comments":true,"path":"2019/11/01/Hadoop-HDFS/","link":"","permalink":"http://yoursite.com/2019/11/01/Hadoop-HDFS/","excerpt":"Hadoop分布式文件系统——HDFS一、介绍HDFS （Hadoop Distributed File System）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。二、HDFS 设计原理 2.1 HDFS 架构HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成：","text":"Hadoop分布式文件系统——HDFS一、介绍HDFS （Hadoop Distributed File System）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。二、HDFS 设计原理 2.1 HDFS 架构HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成： NameNode : 负责执行有关 文件系统命名空间 的操作，例如打开，关闭、重命名文件和目录等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。 DataNode：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。 2.2 文件系统命名空间HDFS 的 文件系统命名空间 的层次结构与大多数文件系统类似 (如 Linux)， 支持目录和文件的创建、移动、删除和重命名等操作，支持配置用户和访问权限，但不支持硬链接和软连接。NameNode 负责维护文件系统名称空间，记录对名称空间或其属性的任何更改。 2.3 数据复制由于 Hadoop 被设计运行在廉价的机器上，这意味着硬件是不可靠的，为了保证容错性，HDFS 提供了数据复制机制。HDFS 将每一个文件存储为一系列块，每个块由多个副本来保证容错，块的大小和复制因子可以自行配置（默认情况下，块大小是 128M，默认复制因子是 3）。 2.4 数据复制的实现原理大型的 HDFS 实例在通常分布在多个机架的多台服务器上，不同机架上的两台服务器之间通过交换机进行通讯。在大多数情况下，同一机架中的服务器间的网络带宽大于不同机架中的服务器之间的带宽。因此 HDFS 采用机架感知副本放置策略，对于常见情况，当复制因子为 3 时，HDFS 的放置策略是： 在写入程序位于 datanode 上时，就优先将写入文件的一个副本放置在该 datanode 上，否则放在随机 datanode 上。之后在另一个远程机架上的任意一个节点上放置另一个副本，并在该机架上的另一个节点上放置最后一个副本。此策略可以减少机架间的写入流量，从而提高写入性能。 如果复制因子大于 3，则随机确定第 4 个和之后副本的放置位置，同时保持每个机架的副本数量低于上限，上限值通常为 （复制系数 - 1）/机架数量 + 2，需要注意的是不允许同一个 dataNode 上具有同一个块的多个副本。 2.5 副本的选择为了最大限度地减少带宽消耗和读取延迟，HDFS 在执行读取请求时，优先读取距离读取器最近的副本。如果在与读取器节点相同的机架上存在副本，则优先选择该副本。如果 HDFS 群集跨越多个数据中心，则优先选择本地数据中心上的副本。 2.6 架构的稳定性1. 心跳机制和重新复制每个 DataNode 定期向 NameNode 发送心跳消息，如果超过指定时间没有收到心跳消息，则将 DataNode 标记为死亡。NameNode 不会将任何新的 IO 请求转发给标记为死亡的 DataNode，也不会再使用这些 DataNode 上的数据。 由于数据不再可用，可能会导致某些块的复制因子小于其指定值，NameNode 会跟踪这些块，并在必要的时候进行重新复制。 2. 数据的完整性由于存储设备故障等原因，存储在 DataNode 上的数据块也会发生损坏。为了避免读取到已经损坏的数据而导致错误，HDFS 提供了数据完整性校验机制来保证数据的完整性，具体操作如下： 当客户端创建 HDFS 文件时，它会计算文件的每个块的 校验和，并将 校验和 存储在同一 HDFS 命名空间下的单独的隐藏文件中。当客户端检索文件内容时，它会验证从每个 DataNode 接收的数据是否与存储在关联校验和文件中的 校验和 匹配。如果匹配失败，则证明数据已经损坏，此时客户端会选择从其他 DataNode 获取该块的其他可用副本。 3.元数据的磁盘故障FsImage 和 EditLog 是 HDFS 的核心数据，这些数据的意外丢失可能会导致整个 HDFS 服务不可用。为了避免这个问题，可以配置 NameNode 使其支持 FsImage 和 EditLog 多副本同步，这样 FsImage 或 EditLog 的任何改变都会引起每个副本 FsImage 和 EditLog 的同步更新。 4.支持快照快照支持在特定时刻存储数据副本，在数据意外损坏时，可以通过回滚操作恢复到健康的数据状态。 三、HDFS 的特点3.1 高容错由于 HDFS 采用数据的多副本方案，所以部分硬件的损坏不会导致全部数据的丢失。 3.2 高吞吐量HDFS 设计的重点是支持高吞吐量的数据访问，而不是低延迟的数据访问。 3.3 大文件支持HDFS 适合于大文件的存储，文档的大小应该是是 GB 到 TB 级别的。 3.3 简单一致性模型HDFS 更适合于一次写入多次读取 (write-once-read-many) 的访问模型。支持将内容追加到文件末尾，但不支持数据的随机访问，不能从文件任意位置新增数据。 3.4 跨平台移植性HDFS 具有良好的跨平台移植性，这使得其他大数据计算框架都将其作为数据持久化存储的首选方案。 附：图解HDFS存储原理 说明：以下图片引用自博客：翻译经典 HDFS 原理讲解漫画 1. HDFS写数据原理 2. HDFS读数据原理 3. HDFS故障类型和其检测方法 第二部分：读写故障的处理 第三部分：DataNode 故障处理 副本布局策略： 参考资料 Apache Hadoop 2.9.2 &gt; HDFS Architecture Tom White . hadoop 权威指南 [M] . 清华大学出版社 . 2017. 翻译经典 HDFS 原理讲解漫画","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/categories/Hadoop/"}],"tags":[{"name":"-HDFS","slug":"HDFS","permalink":"http://yoursite.com/tags/HDFS/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/categories/Hadoop/"}]},{"title":"Hbase常用Shell命令","slug":"Hbase_Shell","date":"2019-11-01T15:45:54.072Z","updated":"2019-11-02T12:05:21.280Z","comments":true,"path":"2019/11/01/Hbase_Shell/","link":"","permalink":"http://yoursite.com/2019/11/01/Hbase_Shell/","excerpt":"Hbase 常用 Shell 命令一、基本命令打开 Hbase Shell：1# hbase shell1.1 获取帮助1234# 获取帮助help# 获取命令的详细信息help 'status'1.2 查看服务器状态1status1.3 查看版本信息1version二、关于表的操作2.1 查看所有表","text":"Hbase 常用 Shell 命令一、基本命令打开 Hbase Shell：1# hbase shell1.1 获取帮助1234# 获取帮助help# 获取命令的详细信息help 'status'1.2 查看服务器状态1status1.3 查看版本信息1version二、关于表的操作2.1 查看所有表 1list 2.2 创建表 命令格式： create ‘表名称’, ‘列族名称 1’,’列族名称 2’,’列名称 N’ 12# 创建一张名为Student的表,包含基本信息（baseInfo）、学校信息（schoolInfo）两个列族create 'Student','baseInfo','schoolInfo' 2.3 查看表的基本信息 命令格式：desc ‘表名’ 1describe 'Student' 2.4 表的启用/禁用enable 和 disable 可以启用/禁用这个表,is_enabled 和 is_disabled 来检查表是否被禁用 12345678# 禁用表disable 'Student'# 检查表是否被禁用is_disabled 'Student'# 启用表enable 'Student'# 检查表是否被启用is_enabled 'Student' 2.5 检查表是否存在1exists 'Student' 2.6 删除表1234# 删除表前需要先禁用表disable 'Student'# 删除表drop 'Student' 三、增删改3.1 添加列族 命令格式： alter ‘表名’, ‘列族名’ 1alter 'Student', 'teacherInfo' 3.2 删除列族 命令格式：alter ‘表名’, {NAME =&gt; ‘列族名’, METHOD =&gt; ‘delete’} 1alter 'Student', &#123;NAME =&gt; 'teacherInfo', METHOD =&gt; 'delete'&#125; 3.3 更改列族存储版本的限制默认情况下，列族只存储一个版本的数据，如果需要存储多个版本的数据，则需要修改列族的属性。修改后可通过 desc 命令查看。 1alter 'Student',&#123;NAME=&gt;'baseInfo',VERSIONS=&gt;3&#125; 3.4 插入数据 命令格式：put ‘表名’, ‘行键’,’列族:列’,’值’ 注意：如果新增数据的行键值、列族名、列名与原有数据完全相同，则相当于更新操作 12345678910111213141516171819put 'Student', 'rowkey1','baseInfo:name','tom'put 'Student', 'rowkey1','baseInfo:birthday','1990-01-09'put 'Student', 'rowkey1','baseInfo:age','29'put 'Student', 'rowkey1','schoolInfo:name','Havard'put 'Student', 'rowkey1','schoolInfo:localtion','Boston'put 'Student', 'rowkey2','baseInfo:name','jack'put 'Student', 'rowkey2','baseInfo:birthday','1998-08-22'put 'Student', 'rowkey2','baseInfo:age','21'put 'Student', 'rowkey2','schoolInfo:name','yale'put 'Student', 'rowkey2','schoolInfo:localtion','New Haven'put 'Student', 'rowkey3','baseInfo:name','maike'put 'Student', 'rowkey3','baseInfo:birthday','1995-01-22'put 'Student', 'rowkey3','baseInfo:age','24'put 'Student', 'rowkey3','schoolInfo:name','yale'put 'Student', 'rowkey3','schoolInfo:localtion','New Haven'put 'Student', 'wrowkey4','baseInfo:name','maike-jack' 3.5 获取指定行、指定行中的列族、列的信息123456# 获取指定行中所有列的数据信息get 'Student','rowkey3'# 获取指定行中指定列族下所有列的数据信息get 'Student','rowkey3','baseInfo'# 获取指定行中指定列的数据信息get 'Student','rowkey3','baseInfo:name' 3.6 删除指定行、指定行中的列1234# 删除指定行delete 'Student','rowkey3'# 删除指定行中指定列的数据delete 'Student','rowkey3','baseInfo:name' 四、查询hbase 中访问数据有两种基本的方式： 按指定 rowkey 获取数据：get 方法； 按指定条件获取数据：scan 方法。 scan 可以设置 begin 和 end 参数来访问一个范围内所有的数据。get 本质上就是 begin 和 end 相等的一种特殊的 scan。 4.1Get查询123456# 获取指定行中所有列的数据信息get 'Student','rowkey3'# 获取指定行中指定列族下所有列的数据信息get 'Student','rowkey3','baseInfo'# 获取指定行中指定列的数据信息get 'Student','rowkey3','baseInfo:name' 4.2 查询整表数据1scan 'Student' 4.3 查询指定列簇的数据1scan 'Student', &#123;COLUMN=&gt;'baseInfo'&#125; 4.4 条件查询12# 查询指定列的数据scan 'Student', &#123;COLUMNS=&gt; 'baseInfo:birthday'&#125; 除了列 （COLUMNS） 修饰词外，HBase 还支持 Limit（限制查询结果行数），STARTROW（ROWKEY 起始行，会先根据这个 key 定位到 region，再向后扫描）、STOPROW(结束行)、TIMERANGE（限定时间戳范围）、VERSIONS（版本数）、和 FILTER（按条件过滤行）等。 如下代表从 rowkey2 这个 rowkey 开始，查找下两个行的最新 3 个版本的 name 列的数据： 1scan 'Student', &#123;COLUMNS=&gt; 'baseInfo:name',STARTROW =&gt; 'rowkey2',STOPROW =&gt; 'wrowkey4',LIMIT=&gt;2, VERSIONS=&gt;3&#125; 4.5 条件过滤Filter 可以设定一系列条件来进行过滤。如我们要查询值等于 24 的所有数据： 1scan 'Student', FILTER=&gt;\"ValueFilter(=,'binary:24')\" 值包含 yale 的所有数据： 1scan 'Student', FILTER=&gt;\"ValueFilter(=,'substring:yale')\" 列名中的前缀为 birth 的： 1scan 'Student', FILTER=&gt;\"ColumnPrefixFilter('birth')\" FILTER 中支持多个过滤条件通过括号、AND 和 OR 进行组合： 12# 列名中的前缀为birth且列值中包含1998的数据scan 'Student', FILTER=&gt;\"ColumnPrefixFilter('birth') AND ValueFilter ValueFilter(=,'substring:1998')\" PrefixFilter 用于对 Rowkey 的前缀进行判断： 1scan 'Student', FILTER=&gt;\"PrefixFilter('wr')\"","categories":[{"name":"Hbase","slug":"Hbase","permalink":"http://yoursite.com/categories/Hbase/"}],"tags":[{"name":"-Hbase命令","slug":"Hbase命令","permalink":"http://yoursite.com/tags/Hbase%E5%91%BD%E4%BB%A4/"}],"keywords":[{"name":"Hbase","slug":"Hbase","permalink":"http://yoursite.com/categories/Hbase/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-11-01T10:48:16.922Z","updated":"2019-11-01T10:48:16.922Z","comments":true,"path":"2019/11/01/hello-world/","link":"","permalink":"http://yoursite.com/2019/11/01/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[],"keywords":[]}]}